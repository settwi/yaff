{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes MCMC spectral fitter: fit a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import numpy as np\n",
    "from yaff import fitting, rebin_flux, plotting as yap\n",
    "import scipy.stats as st\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('nice.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fake data (counts, errors, etc) for fake spectroscopy\n",
    "This data is just a line of counts made with `np.linspace`.\n",
    "It is sampled as a normal distribution assuming $\\sqrt N$ errors.\n",
    "\n",
    "The response matrix starts as diagonal and then gets interpolated to allow different-sized count\n",
    "vs photon energy bins. The interpolation preserves \"probability flux\" along the appropriate\n",
    "response matrix axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "cts = np.linspace(800, 3000, 5) << u.ct\n",
    "cts_err = np.sqrt(cts.value) << u.ct\n",
    "\n",
    "# Add a gaussian approximation of Poisson error onto the counts\n",
    "cts = st.norm.rvs(loc=cts, scale=cts_err) << u.ct\n",
    "eff_exp = 2 << u.s\n",
    "\n",
    "count_edges = [2, 4, 6, 8, 10, 12] << u.keV\n",
    "photon_edges = np.linspace(1, 50, num=40) << u.keV\n",
    "\n",
    "# the SRM needs to get interpolated along the\n",
    "# `target` aka counts axis\n",
    "diag_srm = np.eye(photon_edges.size - 1)\n",
    "\n",
    "# the rows indicate counts\n",
    "# cols are photon energy bin entries\n",
    "# assuming mtarix multiplication S\\vec{p} = \\vec{c}\n",
    "# p means photon, c means model (vectors)\n",
    "srm = list()\n",
    "for row in diag_srm:\n",
    "    interp = rebin_flux.flux_conserving_rebin(\n",
    "        photon_edges, row, count_edges\n",
    "    )\n",
    "    srm.append(interp)\n",
    "\n",
    "\n",
    "srm = np.array(srm).T << (u.ct / u.ph)\n",
    "area = 1 << u.cm**2\n",
    "\n",
    "pack = fitting.DataPacket(\n",
    "    counts=cts,\n",
    "    counts_error=cts_err,\n",
    "    background_counts=0 * cts,\n",
    "    background_counts_error=0 * cts,\n",
    "    effective_exposure=eff_exp,\n",
    "    count_energy_edges=count_edges,\n",
    "    photon_energy_edges=photon_edges,\n",
    "    response_matrix=(area * srm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model to fit\n",
    "Here, we just assume we're gonna fit a line, because we are.\n",
    "THe model accepts a `dict` of arguments:\n",
    "- Photon energy edges\n",
    "- Parameters from the fitter (a `dict[str, yafp.Parameter]`)\n",
    "\n",
    "These are used to compute the model.\n",
    "\n",
    "Inside the model function, you can restrict or \"tie\" certain parameters to one another.\n",
    "If you were fitting two lines and wanted to keep the intercepts the same, for instance,\n",
    "this could be enforced in the model function.\n",
    "\n",
    "The model is also just a pure Python function with very little wrapping it.\n",
    "This gives flexibility to fit any kind of model you'd like.\n",
    "It could even be a method bound to an instance of an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_model(arg_dict: dict[str, object]):\n",
    "    ''' Fit a line to the data we get '''\n",
    "    ph_edges = arg_dict['photon_energy_edges']\n",
    "    params = arg_dict['parameters']\n",
    "\n",
    "    midpoints = ph_edges[:-1] + np.diff(ph_edges)/2\n",
    "    return params['intercept'].value + (params['slope'].value*midpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability functions: log likelihood and log priors\n",
    "The next step is to define a likelihood you would like to use and enforce some prior knowledge on your parameters.\n",
    "\n",
    "The probability function which gets sampled by `emcee` is the [(log) posterior](https://en.wikipedia.org/wiki/Posterior_probability).\n",
    "We use the log of the probability so that there is more granularity in the probability fluctuations.\n",
    "\n",
    "Here we use a $\\chi^2$ log likelihood which works in a lot of cases.\n",
    "You can also use a Poisson or negative binomial likelihood by using e.g.\n",
    "[`st.poisson.logpdf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(data: fitting.DataPacket, model: np.ndarray):\n",
    "    '''Basic chi2 log likelihood'''\n",
    "    return -np.nan_to_num((data.counts - model)**2 / data.counts_error**2).sum()\n",
    "\n",
    "\n",
    "# Define the model parameters we want to use\n",
    "params = {\n",
    "    'slope': fitting.Parameter(-10 << u.ph / u.keV, frozen=False),\n",
    "    'intercept': fitting.Parameter(3000 << u.ph, frozen=False)\n",
    "}\n",
    "\n",
    "# Define the priors on those parameters (uniform from -1000 to 1000 for each)\n",
    "log_priors = {\n",
    "    'slope': fitting.simple_bounds(-10000, 10000),\n",
    "    'intercept': fitting.simple_bounds(-10000, 10000),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual fitting: make sure stuff works\n",
    "Now that the mathematics defining the model have been set up, fitting is straightforward.\n",
    "The fitter coordinates parameter variations and basic `emcee.EnsembleSampler` management.\n",
    "It also facilitates easy conversion from a photon to count model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = fitting.BayesFitter(\n",
    "    data=pack,\n",
    "    model_function=line_model,\n",
    "    parameters=params,\n",
    "    log_priors=log_priors,\n",
    "    log_likelihood=log_likelihood\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the parameters to check if they're in good shape\n",
    "fitter.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yap.plot_data_model(fitter)\n",
    "# Initial comparison of model to data\n",
    "# fig, ax = plt.subplots(layout='constrained')\n",
    "# ax.stairs(pack.counts, pack.count_energy_edges, label='data')\n",
    "# \n",
    "# # The fitter will multiply the response matrix etc\n",
    "# mod = fitter.eval_model()\n",
    "# ax.stairs(mod, pack.count_energy_edges, label='initial model guess')\n",
    "# \n",
    "# ax.legend()\n",
    "# \n",
    "# ax.set(xlabel='Energy (keV)', ylabel='Counts (ct)', title='A line of counts and a model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that the initial guess is horrendous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, perform the fit and give emcee some kwargs if you want\n",
    "fitter.perform_fit(\n",
    "    emcee_constructor_kw=dict(),\n",
    "    emcee_run_kw=dict(nsteps=10000, progress=True)\n",
    ")\n",
    "\n",
    "# Optionally save the fit result to a compressed pickle file\n",
    "# fitter.save('test.pkl.xz', open_func=lzma.open)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics: autocorrelation and MCMC chains\n",
    "It's always a good idea to make sure your fit has \"enough\" samples; this can be assessed by looking at the autocorrelation time of the parameter chains.\n",
    "\n",
    "Oftentimes in X-ray spectroscopy the autocorrelation time is very long because the parameters are strongly correlated. It is not always possible to make `emcee` happy, but it's good to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No error thrown means that the autocorrelation time is much shorter\n",
    "# than the MCMC chain length\n",
    "print('autocorrelation times:', fitter.emcee_sampler.get_autocorr_time())\n",
    "slope_chain, inter_chain = fitter.emcee_sampler.flatchain.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the chains to see how well things converged\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "yap.plot_parameter_chains(fitter, fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, plot some sample models over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "yap.plot_data_model(fitter, num_model_samples=100, fig=fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
